{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0g381XzeCyz"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels: [0 1]\n",
      "Test Labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Train Labels:\", np.unique(y_train))\n",
    "print(\"Test Labels:\", np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy6n-uM2eCy2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    1,   14,   22,   16,   43,  530,\n",
       "        973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,   36,\n",
       "        256,    5,   25,  100,   43,  838,  112,   50,  670,    2,    9,\n",
       "         35,  480,  284,    5,  150,    4,  172,  112,  167,    2,  336,\n",
       "        385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,\n",
       "          4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,    4,\n",
       "       1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,\n",
       "         38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,\n",
       "         16,  626,   18,    2,    5,   62,  386,   12,    8,  316,    8,\n",
       "        106,    5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,\n",
       "        130,   12,   16,   38,  619,    5,   25,  124,   51,   36,  135,\n",
       "         48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,\n",
       "          5,   14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,\n",
       "         15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout \n",
    "from tensorflow.keras.layers import Bidirectional, Lambda, Flatten, Input, Add\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 20)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               96800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 297,001\n",
      "Trainable params: 297,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.4814 - accuracy: 0.7655 - val_loss: 0.3958 - val_accuracy: 0.8208\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.3220 - accuracy: 0.8681 - val_loss: 0.3390 - val_accuracy: 0.8591\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2272 - accuracy: 0.9130 - val_loss: 0.3248 - val_accuracy: 0.8703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff53c65eb00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 20)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               96800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 297,001\n",
      "Trainable params: 297,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model2.add(Bidirectional(LSTM(100)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.5234 - accuracy: 0.7304 - val_loss: 0.3612 - val_accuracy: 0.8473\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2956 - accuracy: 0.8806 - val_loss: 0.3271 - val_accuracy: 0.8605\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2284 - accuracy: 0.9133 - val_loss: 0.3608 - val_accuracy: 0.8581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff460d31e80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 20)           200000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               96800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 297,001\n",
      "Trainable params: 297,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model3.add(Bidirectional(LSTM(100)))\n",
    "model3.add(Dropout(0.50))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.4563 - accuracy: 0.7820 - val_loss: 0.3842 - val_accuracy: 0.8399\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2897 - accuracy: 0.8883 - val_loss: 0.3337 - val_accuracy: 0.8593\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2372 - accuracy: 0.9115 - val_loss: 0.3318 - val_accuracy: 0.8691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2c6f841d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 20)           200000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,501\n",
      "Trainable params: 248,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model4.add(LSTM(100))\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.4618 - accuracy: 0.7716 - val_loss: 0.3428 - val_accuracy: 0.8578\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 28s 1ms/sample - loss: 0.2855 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8484\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 28s 1ms/sample - loss: 0.2076 - accuracy: 0.9226 - val_loss: 0.3201 - val_accuracy: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2a7bcfeb8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 20)           200000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               48400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,501\n",
      "Trainable params: 248,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(vocab_size, 20, input_length=maxlen))\n",
    "model5.add(LSTM(100))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 32s 1ms/sample - loss: 0.4502 - accuracy: 0.7806 - val_loss: 0.3292 - val_accuracy: 0.8624\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 29s 1ms/sample - loss: 0.4595 - accuracy: 0.7764 - val_loss: 0.3979 - val_accuracy: 0.8354\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 29s 1ms/sample - loss: 0.3116 - accuracy: 0.8773 - val_loss: 0.3341 - val_accuracy: 0.8634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff29ed16320>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(x_train, y_train, batch_size=32, epochs=3, verbose=1, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "inp = model.input[100]                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functors = [K.function([inp, K.learning_phase()], outputs)]    # evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'embedding/Identity:0' shape=(None, 300, 20) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional/Identity:0' shape=(None, 200) dtype=float32>,\n",
       " <tf.Tensor 'dense/Identity:0' shape=(None, 1) dtype=float32>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.backend.EagerExecutionFunction at 0x7ff50c1a5320>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AqOnLa2eCzH"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dUDSg7VeCzM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33181426030158995, 0.86908]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tskt_1npeCzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing few predictions:\n",
      "Actual Label: 1\n",
      "Predicted Label: [0.0206005]\n",
      "Actual Label: 1\n",
      "Predicted Label: [0.82928574]\n",
      "Actual Label: 0\n",
      "Predicted Label: [0.0839752]\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing few predictions:\")\n",
    "print(\"Actual Label:\", y_test[100])\n",
    "print(\"Predicted Label:\", predictions[100])\n",
    "print(\"Actual Label:\", y_test[10])\n",
    "print(\"Predicted Label:\", predictions[10])\n",
    "print(\"Actual Label:\", y_test[9999])\n",
    "print(\"Predicted Label:\", predictions[9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
